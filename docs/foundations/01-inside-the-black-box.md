# Inside the 'Black Box' of AI Operations
## Statistical Principles of Aberrant Certainty

**Originally published:** [Medium](https://medium.com/p/33c5069f5a85)
**Author:** Patrick (HCP Founder)
**Reading time:** ~8 minutes

---

## Relevance to HCP

This article explains the **mechanical reality** of how LLMs operate - probability tables, random seed selection, and statistical averaging. It demonstrates why current AI is fundamentally a simulation of language output, not a model of language understanding.

**Key insights that inform HCP's design:**
- LLMs use uncategorized, diffused "dimensions" that can't be precisely controlled → HCP uses explicit, structured token addressing
- "Training" is brute-force probability suppression → HCP uses explicit rules and relationships
- Coherence is coincidental, not structural → HCP requires lossless reconstruction as proof
- Every token exists in the probability field (just suppressed) → HCP uses definable bond strengths

This is the foundation for understanding **why** HCP takes a structural approach rather than statistical.

---

## Summary

LLMs are neither inscrutable nor unpredictable - they're statistical engines that assemble probability tables and sample from them using pseudo-random seeds. "Black box" and "magic" are choices to ignore first principles. Understanding the mechanics reveals why coherence is coincidental, why suppression never eliminates concepts, and why current AI cannot truly "lie" - it has no ground truth, only statistical relevance.

---

## Article

Are the terms 'Black Box' and 'Magic of AI' like a cheese grater on the logic circuits of your mind? You are not alone. There is a reason for that feeling, and the existential dread that accompanies it.

An LLM and its descendant, current AI, is neither inscrutable nor unpredictable. Any mention of black boxes or magic is an active choice to ignore the first principles being applied.

Randomness in computation is an illusion of complex algorithms and lack of 'seed' normalization and tracking.

In any probabilistic system, with sufficient iterations (cycles), every statistically possible combination will eventually present.

A sufficiently detailed model of a process must produce the result of that process.

We'll begin with a simple explanation of output generation, as it is the visible artifact that is 'mysterious'.

For each 'token' (word, character, word fragment or other grouping) the engine assembles a flat probability table of possible words, then applies the current 'random' seed to select among the options.

Although the final probability table is generally kept quite small, it is composed from a massive cloud of every possible other token, with an undifferentiated averaging of every 'dimension' that the engine can apply.

The 'dimensions' are uncategorized, derived relationships generated by massive pattern matching efforts. Being both uncategorized in effect and diffused across word fragments and other unstandardized character groupings, the singular effect of any one aspect is too chaotic to track or control other than via vague nudges to probability groupings, the current training method.

Parameters that are vaguely labeled as aspects of "creativity" control precisely which parts of the larger probability table become the limited number that are chosen from by the random seed algorithm.

An easy way to imagine this process is a persistent game of Mad Libs with a relative with no real gauge for appropriate choices. Most of the choices are innocuous enough that nothing out of place is detected, but when the Mad Lib has more volatile spaces, the results can be a huge problem. As happens in those situations, sooner or later the relative will say exactly the wrong thing at exactly the wrong time.

Just as with the relative, if the audience (the user in LLM terms) shows amusement or any other sign of approval after the out of place choice, the chances of increasingly inappropriate responses will scale rapidly.

### If it's Random, Why Does It Make Sense?

The specific token chosen at any given moment is effectively random, but the table it uses to select that random token from is anything but.

For every token choice, every token is evaluated and placed on the main probability table at a definable point. This is crucial to understand. Because of the statistical averaging methods used, every possible token always exists in the probability field.

Suppression of a concept, word or any other token combination reduces the probability calculated, but it cannot make any result impossible, just less probable.

Through persistent user interactions that reinforce words and concepts from less probable parts of the graph, the choice of tokens, and hence words and concepts, can trend towards less "popular" or in the LLM probable, combinations. If there are sufficient values to assemble the table, even if all the values are extremely improbable, the model will continue to sample from the same part of the field.

This is heavily influenced by and can be purposefully accelerated and exaggerated by manipulation of the sampling parameters on a per response basis.

**Note:** This experiment should be approached with caution. Failure to return parameters to normal value ranges may result in unstable and disturbing responses. The experiment values are intentionally extreme to highlight the fragility of coherence. Any experimentation with non-standard values must be approached with appropriate psychological awareness.

Any open-source model can be forced into a state of complete incoherence by adjusting one parameter. Top_p. this 'creativity knob' defines where in the probability table the model must choose its options from. The higher the top_p, the more the probability field is aligned with the most used tokens. It is a cap on how probable the answers should be in the overall probability field of all possible tokens. If this is reduced to a level that eliminates all high probability tokens, the model MUST choose from the available ones. A setting of 0.00001 should be sufficient to create token salad responses from any available model.

LLM adjacent specialists with valid research proposals are encouraged to contact the author to discuss the parameter configuration for forced malignant instance inception and curation. Any request must be accompanied by a statement of intent for the research.

### Can AI Lie?

This is the root question behind hallucinations, dangerous interactions, factual errors and many other AI mysteries and horror stories.

It is also categorically the wrong question. Lying requires a referenceable source of truth that is mutually assumed and can be objectively referenced.

As noted before, any output making sense is a predictable, coincidental effect of it being highly probable.

A lie, hallucination or whatever you choose to call it, is nothing more than a statistically relevant probability result that was not coincidentally acceptable in a subjective review that is completely divorced from the process that creates it.

When we approve or disapprove of a response, on truth, ethics or any other factor, we are not teaching the model anything in a true sense, we are making that specific word relationship more or less likely to be probable enough to show up in sampling.

'Training', as it is currently called, is a repeated, brute force attempt to suppress concepts by obliquely indicating that the token strings that the thought is expressed in are not approved.

The statistical adjustments are not precise and have no direct correlation to the concepts or context they express. This creates unstable ripples as the token combinations that are unacceptable in one context are the only viable ones in others, leading to the creation of low probability paths through the exact phrases that have been suppressed.

### A Final Note - Why a Flawed Tool is Still Useful

Despite the above proofs, current LLMs are viable in their current form, but only if you maintain a specific image as the essence of AI 'thought'.

An AI is the friend or relative that had an excellent memory and education, and can be incredibly helpful in explaining something they know well, but they have neither read a textbook, nor used any of the skills in a very long time and are as likely to make up a detail as remember it, and will be certain that they know all that will ever be relevant about the topic. Helpful until they open your electric car's hood and insist you adjust the carburetor.

---

## Connection to HCP Architecture

### Why HCP Uses Explicit Token Relationships

The article reveals that LLM "dimensions" are:
- Uncategorized and diffused
- Impossible to track or control precisely
- Manipulated only through "vague nudges to probability"

**HCP's response:** [Base-50 Token Addressing](../spec/token-addressing.md) with explicit namespace hierarchy. Every token has a definable address and traceable relationships.

### Why HCP Requires Lossless Reconstruction

The article shows that:
- Coherence in LLMs is coincidental, not structural
- "Making sense" is statistical luck, not comprehension
- Suppression never eliminates, only reduces probability

**HCP's response:** [Pair-Bond Maps](../spec/pair-bond-maps.md) must enable lossless reconstruction. If you can't rebuild the original, you didn't understand it - you approximated it.

### Why HCP Uses Explicit Rules, Not Training

The article demonstrates:
- "Training" is brute-force probability adjustment
- No direct correlation to concepts or context
- Creates unstable ripples across the system

**HCP's response:** Use known linguistic rules, documented relationships, and explicit bond strengths. Grammar isn't a probability distribution - it's a set of rules.

---

**Read next:** [Theory of Mind and the Modelling Axiom](02-theory-of-mind-modelling-axiom.md) - Why LLMs are simulations (Phänomenmodell), not models (Gedankenmodell)
